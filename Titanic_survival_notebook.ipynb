{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-70262e3cc250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# database processing package similar to SQL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Common Machine Learning Algorithms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# This is the library import session.\n",
    "import sys # system parameter\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd # database processing package similar to SQL\n",
    "\n",
    "# Common Machine Learning Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, \\\n",
    "naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "#from xgboost import XGBClassifier missing wait for add\n",
    "#Common Model Helper package\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_test = train_df.copy(deep = True)\n",
    "data_cleaner = [train_test, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preview and exploration\n",
    "train_df.info()\n",
    "train_df.sample(10)\n",
    "train_df.groupby('Survived').count()\n",
    "print('Train Columns with null:\\n', train_test.isnull().sum())\n",
    "print(\"-\" * 10)\n",
    "\n",
    "print('Test/Validation columns with null:\\n', test_df.isnull().sum())\n",
    "print(\"-\" * 10)\n",
    "\n",
    "train_df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "for dataset in data_cleaner:\n",
    "    # fill the missing data with median\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "    \n",
    "    #Complete Embark information with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True) #mode is the number with largest frequency\n",
    "    #complete the missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "drop_column = ['PassengerId', 'Cabin', 'Ticket']\n",
    "train_test.drop(drop_column, axis=1, inplace=True)\n",
    "    \n",
    "print(train_test.isnull().sum())\n",
    "print(\"-\" * 10)\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Feature engineering for train and test/validation dataset\n",
    "for dataset in data_cleaner:\n",
    "    # count the total number of family number\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    \n",
    "    dataset['IsAlone'] = 1 # use 1 to denote alone\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n",
    "    # filter out the title like Mr. Ms. Miss etc\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1]\\\n",
    "    .str.split(\".\", expand = True)[0]\n",
    "    # catogorize using the frequency distribution 0%, 25%, 50%, 75%, 100%\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "    #catogorize using the age range evenly separated into 5\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "\n",
    "stat_min = 10\n",
    "title_names = (train_test['Title'].value_counts() < stat_min)\n",
    "    \n",
    "train_test['Title'] = train_test['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "print(train_test['Title'].value_counts())\n",
    "print(\"-\" * 10)\n",
    "    \n",
    "train_test.info()\n",
    "test_df.info()\n",
    "train_test.sample(10)\n",
    "print(train_test['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert objects to category using label encoder for train\n",
    "# test/validation dataset\n",
    "\n",
    "label = LabelEncoder()\n",
    "for dataset in data_cleaner:\n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "    \n",
    "\n",
    "# define target variable\n",
    "Target = ['Survived']\n",
    "# Input variable aka feature selection\n",
    "\n",
    "train_test_x = ['Sex', 'Pclass','Embarked', 'Title','SibSp'\n",
    "                , 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] # original column name in charts\n",
    "train_test_x_code = ['Sex_Code','Pclass', 'Embarked_Code', \n",
    "                   'Title_Code','SibSp', 'Parch', 'Age', \n",
    "                   'Fare'] # coded column names for algorithm\n",
    "train_test_xy = Target + train_test_x\n",
    "print('Original X Y: ', train_test_xy, '\\n')\n",
    "\n",
    "#define x variables for original w/bin features to remove continuous variables\n",
    "train_test_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "train_test_xy_bin = Target + train_test_x_bin\n",
    "print('Bin X Y: ', train_test_xy_bin, '\\n')\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "train_test_dummy = pd.get_dummies(train_test[train_test_x])\n",
    "train_test_x_dummy = train_test_dummy.columns.tolist()\n",
    "train_test_xy_dummy = Target + train_test_x_dummy\n",
    "print('Dummy X Y: ', train_test_xy_dummy, '\\n')\n",
    "\n",
    "train_test_dummy.head()\n",
    "train_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the cleaned data\n",
    "print('Train columns with null values: \\n', train_test.isnull().sum())\n",
    "print('-' * 10)\n",
    "print(train_test.info())\n",
    "print(\"-\" * 10)\n",
    "print('Test/Validation columns with null values: \\n', test_df.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (test_df.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "train_df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_x, test1_x, train1_y, test1_y = \\\n",
    "model_selection.train_test_split(train_test[train_test_x_code],\\\n",
    "                                 train_test[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = \\\n",
    "model_selection.train_test_split(train_test[train_test_x_bin], \\\n",
    "                                 train_test[Target], random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy= \\\n",
    "model_selection.train_test_split(train_test_dummy[train_test_x_dummy], \\\n",
    "                                 train_test[Target], random_state = 0)\n",
    "\n",
    "print(\"train_test shape:{}\".format(train_test.shape))\n",
    "print(\"train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"test1 Shape: {}\".format(test1_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Exploratory Analysis with Statisticsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrete Variable Correlation by Survival using\n",
    "#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "for x in train_test_x:\n",
    "    if train_test[x].dtype != 'float64' :\n",
    "        print('Survival Correlation by:', x)\n",
    "        print(train_test[[x, Target[0]]].\n",
    "              groupby(x, as_index = False).mean())\n",
    "        print('-' * 10, '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph distribution of quantitative data\n",
    "plt.figure(figsize=[16,12])\n",
    "plt.subplot(231)\n",
    "plt.boxplot(x = train_test['Fare'], showmeans = True, meanline = True)\n",
    "plt.title('Fare Boxplot')\n",
    "plt.ylabel('Fare ($)')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.boxplot(x = train_test['Age'], showmeans = True, meanline = True)\n",
    "plt.title('Age Boxplot')\n",
    "plt.ylabel('Age (Years)')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.boxplot(train_test['FamilySize'], showmeans = True, meanline = True)\n",
    "plt.title('Family Size Boxplot')\n",
    "plt.ylabel('Family Size (#)')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(x = [train_test[train_test['Survived'] == 1]['Fare'],\n",
    "             train_test[train_test['Survived'] == 0]['Fare']], \n",
    "             stacked = True, color = ['g', 'r'],\n",
    "             label = ['Survived', 'Dead'])\n",
    "plt.title('Fare Histogram by Survival')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(x = [train_test[train_test['Survived']==1]['Age'], train_test[train_test['Survived']==0]['Age']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Age Histogram by Survival')\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(x = [train_test[train_test['Survived']==1]['FamilySize'],\n",
    "              train_test[train_test['Survived']==0]['FamilySize']], \n",
    "              stacked=True, color = ['g','r'], \n",
    "              label = ['Survived','Dead'])\n",
    "plt.title('Family Size Histogram by Survival')\n",
    "plt.xlabel('Family Size (#)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph individual features by survival\n",
    "fig, saxis = plt.subplots(2, 3, figsize = (16, 12))\n",
    "\n",
    "sns.barplot(x = 'Embarked', y = 'Survived',\n",
    "           data = train_test, ax = saxis[0,0])\n",
    "sns.barplot(x = 'Pclass', y = 'Survived', order = [1,2,3],\n",
    "            data=train_test, ax = saxis[0,1])\n",
    "sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0],\n",
    "            data=train_test, ax = saxis[0,2])\n",
    "sns.pointplot(x = 'FareBin', y = 'Survived',\n",
    "              data=train_test, ax = saxis[1,0])\n",
    "sns.pointplot(x = 'AgeBin', y = 'Survived',\n",
    "              data=train_test, ax = saxis[1,1])\n",
    "sns.pointplot(x = 'FamilySize', y = 'Survived',\n",
    "              data=train_test, ax = saxis[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph distribution of qualitative data: Pclass\n",
    "#we know class mattered in survival, now let's compare class and a 2nd feature\n",
    "fig, (axis1, axis2, axis3) = plt.subplots(1, 3, figsize = (14, 12))\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived',\n",
    "           data = train_test, ax = axis1)\n",
    "axis1.set_title('Pclass vs Fare Survival Comparison')\n",
    "\n",
    "sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = train_test, split = True, ax = axis2)\n",
    "axis2.set_title('Pclass vs Age Survival Comparison')\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = train_test, ax = axis3)\n",
    "axis3.set_title('Pclass vs Family Size Survival Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph distribution of qualitative data: Sex\n",
    "#we know sex mattered in survival, now let's compare sex and a 2nd feature\n",
    "fig, qaxis = plt.subplots(1,3,figsize=(14,12))\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=train_test, ax = qaxis[0])\n",
    "axis1.set_title('Sex vs Embarked Survival Comparison')\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=train_test, ax  = qaxis[1])\n",
    "axis1.set_title('Sex vs Pclass Survival Comparison')\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=train_test, ax  = qaxis[2])\n",
    "axis1.set_title('Sex vs IsAlone Survival Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (maxis1, maxis2) = plt.subplots(1, 2, figsize = (14, 12))\n",
    "\n",
    "#how does family size factor with sex & survival compare\n",
    "sns.pointplot(x = 'FamilySize', y = 'Survived', hue = 'Sex',\n",
    "             data = train_test,\n",
    "             palette = {\"male\": \"blue\", \"female\" : \"pink\"},\n",
    "             markers = [\"*\", \"o\"], linestyles = [\"-\", \"--\"],\n",
    "             ax = maxis1)\n",
    "\n",
    "#how does class factor with sex & survival compare\n",
    "sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_test,\n",
    "              palette={\"male\": \"blue\", \"female\": \"pink\"},\n",
    "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how does embark port factor with class, sex, and survival compare\n",
    "e = sns.FacetGrid(train_test, col = 'Embarked')\n",
    "e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex',\n",
    "      ci = 95.0, palette = 'deep') # ci is confidence interval\n",
    "e.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distributions of age of passengers who survived or did not survive\n",
    "a = sns.FacetGrid(train_test, hue = 'Survived', aspect = 4)\n",
    "a.map(sns.kdeplot, 'Age', shade = True)\n",
    "a.set(xlim=(0 , train_test['Age'].max()))\n",
    "a.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = sns.FacetGrid(train_test, row = 'Sex', col = 'Pclass',\n",
    "                  hue = 'Survived')\n",
    "h.map(plt.hist, 'Age', alpha = .75)\n",
    "h.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = sns.pairplot(train_test, hue = 'Survived',\n",
    "                 palette = 'deep', size = 1.2,\n",
    "                 diag_kind = 'kde', diag_kws = dict(shade=True),\n",
    "                 plot_kws=dict(s=10))\n",
    "pp.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df):\n",
    "    _, ax = plt.subplots(figsize = (14, 12))\n",
    "    colormap = sns.diverging_palette(200, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(),\n",
    "        cmap = colormap,\n",
    "        square = True,\n",
    "        cbar_kws = {'shrink':.9 },\n",
    "        ax = ax,\n",
    "        annot = True,\n",
    "        linewidths = 0.1, vmax = 1.0, linecolor = 'White',\n",
    "        annot_kws = {'fontsize':12}\n",
    "        )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y = 1.05, size = 15)\n",
    "    \n",
    "correlation_heatmap(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "    \n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits=10,\n",
    "                                       test_size=.3,\n",
    "                                       train_size=.6,\n",
    "                                       random_state=0)\n",
    "# run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters',\n",
    "               'MLA Train Accuracy Mean', \n",
    "               'MLA Test Accuracy Mean', \n",
    "               'MLA Test Accuracy 3*STD' ,\n",
    "               'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = train_test[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    \n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__ # use the class name as the MLA name\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    # get parameters for specific alg\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    #score model with cross validation: \n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, \n",
    "                                               train_test[train_test_x_bin],\n",
    "                                               train_test[Target],\n",
    "                                               cv = cv_split)\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(train_test[train_test_x_bin], train_test[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(train_test[train_test_x_bin])\n",
    "    row_index += 1\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = 'MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Evaluate Model Performance and Tune Model with Hyper-Parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, train_test[train_test_x_bin], train_test[Target], cv  = cv_split)\n",
    "dtree.fit(train_test[train_test_x_bin], train_test[Target])\n",
    "\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\n",
    "print('-'*10)\n",
    "\n",
    "#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "param_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n",
    "              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n",
    "              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n",
    "              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n",
    "              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n",
    "              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n",
    "              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "tune_model.fit(train_test[train_test_x_bin], train_test[Target])\n",
    "\n",
    "#print(tune_model.cv_results_.keys())\n",
    "#print(tune_model.cv_results_['params'])\n",
    "print('AFTER DT Parameters: ', tune_model.best_params_)\n",
    "#print(tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Model with Feature Selection\n",
    "\n",
    "#base model\n",
    "print('BEFORE DT RFE Training Shape Old: ', train_test[train_test_x_bin].shape) \n",
    "print('BEFORE DT RFE Training Columns Old: ', train_test[train_test_x_bin].columns.values)\n",
    "\n",
    "print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#feature selection by recursive feature elimination(RFE)\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(train_test[train_test_x_bin], train_test[Target])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "X_rfe = train_test[train_test_x_bin].columns.values[dtree_rfe.get_support()] # get the features columns with best score\n",
    "rfe_results = model_selection.cross_validate(dtree, train_test[X_rfe], train_test[Target], cv = cv_split)\n",
    "\n",
    "#print(dtree_rfe.grid_scores_)\n",
    "print('AFTER DT RFE Training Shape New: ', train_test[X_rfe].shape) \n",
    "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#tune rfe model\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "rfe_tune_model.fit(train_test[X_rfe], train_test[Target])\n",
    "\n",
    "#print(rfe_tune_model.cv_results_.keys())\n",
    "#print(rfe_tune_model.cv_results_['params'])\n",
    "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
    "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                                feature_names = data1_x_bin, class_names = True,\n",
    "                                filled = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
